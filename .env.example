# ---------------------------------------------------------------------------
# OpenBrowserClaw - Environment Configuration
# ---------------------------------------------------------------------------
# Copy this file to .env and customize the values
# ---------------------------------------------------------------------------

# ===========================================================================
# App Configuration
# ===========================================================================

# Port to expose the OpenBrowserClaw app on
APP_PORT=5173

# App version (optional)
APP_VERSION=0.1.0



# Uncomment to use PostgreSQL instead of SQLite
# POSTGRES_PASSWORD=your-secure-postgres-password

# ===========================================================================
# Optional: AI Service Proxies
# ===========================================================================

# Ollama local API URL (for local LLM inference)
# On Linux, host.docker.internal is not available. Options:
# - Leave empty to disable Ollama proxy
# - Set to http://localhost:11434 if Ollama is running on the host
# OLLAMA_URL=http://localhost:11434

# OpenWebUI URL (for remote AI service proxy)
# Leave empty to disable, or set to your OpenWebUI instance URL
# Example: OPENWEBUI_URL=http://localhost:8008
# OPENWEBUI_URL=

# ===========================================================================
# Optional: Element Web Client
# ===========================================================================

# Port to expose Element web client (uncomment in docker-compose.yml first)
# ELEMENT_PORT=8080